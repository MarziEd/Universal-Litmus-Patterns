---
layout: default
title: Universal Litmus Patterns
---


<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/teaser.png" width="750" alt style></p>

<div class="spaceafterimg">

<h5 id="abstract"><b>Abstract</b></h5>
<p>The unprecedented success of deep neural networks in many applications has made these networks a prime target for adversarial exploitation. In this paper, we introduce
    a benchmark technique for detecting backdoor attacks (aka Trojan attacks) on deep convolutional neural networks (CNNs). We introduce the concept of Universal Litmus Patterns
    (ULPs), which enable one to reveal backdoor attacks by feeding these universal patterns to the network and analyzing the output (i.e., classifying the network as ‘clean’
    or ‘corrupted’). This detection is fast because it requires only a few forward passes through a CNN. We demonstrate the effectiveness of ULPs for detecting backdoor attacks on
    thousands of networks with different architectures trained on four benchmark datasets, namely the German Traffic Sign Recognition Benchmark (GTSRB), MNIST, CIFAR10,
    and Tiny-ImageNet.</p>

<h5 id="code"><b>Code</b></h5>
<p><a href="https://github.com/UMBCvision/Universal-Litmus-Patterns">https://github.com/UMBCvision/Universal-Litmus-Patterns</a></p>

<h5 id="contributions"><b>Contributions</b></h5>
<p>We introduce a fast benchmark technique for detecting backdoor attacks (aka Trojan attacks) on CNNs.</p>

<p>Universal Litmus Patterns (ULPs):
<li>Are optimized input images for which the network’s output becomes a good indicator of whether the network is clean or poisoned (contains a backdoor).</li>
<li>Are trained only on models and not images (train/test).</li>
<li>Transfer across network architectures.</li>
We provide hundreds of clean and poisoned models per dataset for Tiny-ImageNet, CIFAR10, MNIST, and German Traffic Sign Recognition Benchmark (GTSRB).</p>

<h5 id="threat_model"><b>Threat model</b></h5>
<p>Each poisoned model is trained to contain a single trigger that causes images from the source class to be classified as the target class.
We assume no prior knowledge of the targeted class or the triggers used by attackers.</p>

<p style="text-align:center;"><img src="/assets/images/threat_model.png" width="750" alt style></p>

<h5 id="results"><b>Results</b></h5>
<p>ULPs are fast for detection because each ULP requires just one forward pass through the network. Despite this simplicity, surprisingly, ULPs are
competitive for detecting backdoor attacks, establishing a new performance baseline: area under the ROC curve close to 1 on both CIFAR10 and MNIST, 0.96 on GTSRB (for
ResNet18), and 0.94 on Tiny-ImageNet. </p>
<p>ULPs consistently outperform the baselines, Neural-Cleanse [1] and noise inputs (replace ULPs with random inputs), on all datasets with respect to AUCs,
while being considerably faster than Neural-Cleanse (~90,000 times).</p>

<p style="text-align:center;"><img src="/assets/images/results_table.png" width="750" alt style></p>

<p style="text-align:center;"><img src="/assets/images/results_GTSRB.png" width="750" alt style></p>

<p>On GTSRB, ULPs trained on a specific architecture, e.g., VGG or ResNet, transfer well to similar architectures, i.e., random-VGGs and random-ResNets, where we trained models with a random depth and random number of convolutional kernels. ULPs have limited Transferability between different architectures, e.g., from VGG to ResNet and vice versa. </p>

<p style="text-align:center;"><img src="/assets/images/results_generalizability.png" width="500" alt style></p>

<p style="text-align:center;"><img src="/assets/images/ULPs.png" width="750" alt style></p>


